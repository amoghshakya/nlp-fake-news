{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64f9e9d9-23a8-4d6a-8a88-a392036e5892",
   "metadata": {},
   "source": [
    "# Natural Language Processing - Fake News Detection with LIAR dataset\n",
    "\n",
    "Dataset [source](https://paperswithcode.com/dataset/liar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f4b4f8-d816-4a73-8204-f87f6e8af900",
   "metadata": {},
   "source": [
    "## Import necessary libraries\n",
    "\n",
    "- `pandas` for data manipulation (reading dataset and manipulation)\n",
    "- `nltk` for Natural Language Processing stuff \n",
    "- `re` for applying regular expressions (RegEx)\n",
    "- `sklearn` (scikit-learn) for checking accuracy through accuracy score and for some algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c01442c0-110c-4829-8f95-56586422a294",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\amoghshakya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\amoghshakya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amoghshakya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85dcaa7-9508-4593-828a-0db7601502c3",
   "metadata": {},
   "source": [
    "## Reading the dataset\n",
    "\n",
    "The dataset from LIAR dataset is actually in `tsv` (Tab Separated Values) format. So, we can use the `sep` argument in `pandas.read_csv()` method to read values separated by `\\t` (tabs).\n",
    "\n",
    "> *(preprocessing step)*\n",
    ">\n",
    "> There are no columns in the dataset (bummer). We can assign them columns provided from the README file (of the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e90a005c-5a1e-4fec-993d-ac81aeddb9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset\n",
    "dataset_folder = \"./liar_dataset\"\n",
    "\n",
    "# re-reading for experimenting\n",
    "def read_dataset() -> tuple[3]:\n",
    "    # these are tsv files, so put separator as '\\t'\n",
    "    train_df = pd.read_csv(f\"{dataset_folder}/train.tsv\", sep=\"\\t\")\n",
    "    test_df = pd.read_csv(f\"{dataset_folder}/test.tsv\", sep=\"\\t\")\n",
    "    valid_df = pd.read_csv(f\"{dataset_folder}/valid.tsv\", sep=\"\\t\")\n",
    "\n",
    "    # define columns \n",
    "\n",
    "    columns = [\"id\", \"label\", \"statement\", \"subject\", \"speaker\", \n",
    "               \"speaker_job\", \"state_info\", \"party_affiliation\", \n",
    "               \"barely_true_counts\", \"false_counts\", \"half_true_counts\", \n",
    "               \"mostly_true_counts\", \"pants_on_fire_counts\", \"context\"]\n",
    "    \n",
    "    train_df.columns = columns\n",
    "    test_df.columns = columns\n",
    "    valid_df.columns = columns\n",
    "\n",
    "    return train_df, test_df, valid_df\n",
    "\n",
    "train_df, test_df, valid_df = read_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34096e25-10e2-46ba-a347-45b264cb1fd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>statement</th>\n",
       "      <th>subject</th>\n",
       "      <th>speaker</th>\n",
       "      <th>speaker_job</th>\n",
       "      <th>state_info</th>\n",
       "      <th>party_affiliation</th>\n",
       "      <th>barely_true_counts</th>\n",
       "      <th>false_counts</th>\n",
       "      <th>half_true_counts</th>\n",
       "      <th>mostly_true_counts</th>\n",
       "      <th>pants_on_fire_counts</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10540.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>When did the decline of coal start? It started...</td>\n",
       "      <td>energy,history,job-accomplishments</td>\n",
       "      <td>scott-surovell</td>\n",
       "      <td>State delegate</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>democrat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a floor speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>324.json</td>\n",
       "      <td>mostly-true</td>\n",
       "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
       "      <td>foreign-policy</td>\n",
       "      <td>barack-obama</td>\n",
       "      <td>President</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>democrat</td>\n",
       "      <td>70.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Denver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1123.json</td>\n",
       "      <td>false</td>\n",
       "      <td>Health care reform legislation is likely to ma...</td>\n",
       "      <td>health-care</td>\n",
       "      <td>blog-posting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>7.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>a news release</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9028.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>The economic turnaround started at the end of ...</td>\n",
       "      <td>economy,jobs</td>\n",
       "      <td>charlie-crist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Florida</td>\n",
       "      <td>democrat</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>an interview on CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12465.json</td>\n",
       "      <td>true</td>\n",
       "      <td>The Chicago Bears have had more starting quart...</td>\n",
       "      <td>education</td>\n",
       "      <td>robin-vos</td>\n",
       "      <td>Wisconsin Assembly speaker</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>republican</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>a an online opinion-piece</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10234</th>\n",
       "      <td>5473.json</td>\n",
       "      <td>mostly-true</td>\n",
       "      <td>There are a larger number of shark attacks in ...</td>\n",
       "      <td>animals,elections</td>\n",
       "      <td>aclu-florida</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Florida</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>interview on \"The Colbert Report\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10235</th>\n",
       "      <td>3408.json</td>\n",
       "      <td>mostly-true</td>\n",
       "      <td>Democrats have now become the party of the [At...</td>\n",
       "      <td>elections</td>\n",
       "      <td>alan-powell</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>republican</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>an interview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10236</th>\n",
       "      <td>3959.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>Says an alternative to Social Security that op...</td>\n",
       "      <td>retirement,social-security</td>\n",
       "      <td>herman-cain</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>republican</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>a Republican presidential debate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10237</th>\n",
       "      <td>2253.json</td>\n",
       "      <td>false</td>\n",
       "      <td>On lifting the U.S. Cuban embargo and allowing...</td>\n",
       "      <td>florida,foreign-policy</td>\n",
       "      <td>jeff-greene</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Florida</td>\n",
       "      <td>democrat</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a televised debate on Miami's WPLG-10 against ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10238</th>\n",
       "      <td>1155.json</td>\n",
       "      <td>pants-fire</td>\n",
       "      <td>The Department of Veterans Affairs has a manua...</td>\n",
       "      <td>health-care,veterans</td>\n",
       "      <td>michael-steele</td>\n",
       "      <td>chairman of the Republican National Committee</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>republican</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>a Fox News interview</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10239 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id        label  \\\n",
       "0      10540.json    half-true   \n",
       "1        324.json  mostly-true   \n",
       "2       1123.json        false   \n",
       "3       9028.json    half-true   \n",
       "4      12465.json         true   \n",
       "...           ...          ...   \n",
       "10234   5473.json  mostly-true   \n",
       "10235   3408.json  mostly-true   \n",
       "10236   3959.json    half-true   \n",
       "10237   2253.json        false   \n",
       "10238   1155.json   pants-fire   \n",
       "\n",
       "                                               statement  \\\n",
       "0      When did the decline of coal start? It started...   \n",
       "1      Hillary Clinton agrees with John McCain \"by vo...   \n",
       "2      Health care reform legislation is likely to ma...   \n",
       "3      The economic turnaround started at the end of ...   \n",
       "4      The Chicago Bears have had more starting quart...   \n",
       "...                                                  ...   \n",
       "10234  There are a larger number of shark attacks in ...   \n",
       "10235  Democrats have now become the party of the [At...   \n",
       "10236  Says an alternative to Social Security that op...   \n",
       "10237  On lifting the U.S. Cuban embargo and allowing...   \n",
       "10238  The Department of Veterans Affairs has a manua...   \n",
       "\n",
       "                                  subject         speaker  \\\n",
       "0      energy,history,job-accomplishments  scott-surovell   \n",
       "1                          foreign-policy    barack-obama   \n",
       "2                             health-care    blog-posting   \n",
       "3                            economy,jobs   charlie-crist   \n",
       "4                               education       robin-vos   \n",
       "...                                   ...             ...   \n",
       "10234                   animals,elections    aclu-florida   \n",
       "10235                           elections     alan-powell   \n",
       "10236          retirement,social-security     herman-cain   \n",
       "10237              florida,foreign-policy     jeff-greene   \n",
       "10238                health-care,veterans  michael-steele   \n",
       "\n",
       "                                         speaker_job state_info  \\\n",
       "0                                     State delegate   Virginia   \n",
       "1                                          President   Illinois   \n",
       "2                                                NaN        NaN   \n",
       "3                                                NaN    Florida   \n",
       "4                         Wisconsin Assembly speaker  Wisconsin   \n",
       "...                                              ...        ...   \n",
       "10234                                            NaN    Florida   \n",
       "10235                                            NaN    Georgia   \n",
       "10236                                            NaN    Georgia   \n",
       "10237                                            NaN    Florida   \n",
       "10238  chairman of the Republican National Committee   Maryland   \n",
       "\n",
       "      party_affiliation  barely_true_counts  false_counts  half_true_counts  \\\n",
       "0              democrat                 0.0           0.0               1.0   \n",
       "1              democrat                70.0          71.0             160.0   \n",
       "2                  none                 7.0          19.0               3.0   \n",
       "3              democrat                15.0           9.0              20.0   \n",
       "4            republican                 0.0           3.0               2.0   \n",
       "...                 ...                 ...           ...               ...   \n",
       "10234              none                 0.0           1.0               1.0   \n",
       "10235        republican                 0.0           0.0               0.0   \n",
       "10236        republican                 4.0          11.0               5.0   \n",
       "10237          democrat                 3.0           1.0               3.0   \n",
       "10238        republican                 0.0           1.0               1.0   \n",
       "\n",
       "       mostly_true_counts  pants_on_fire_counts  \\\n",
       "0                     1.0                   0.0   \n",
       "1                   163.0                   9.0   \n",
       "2                     5.0                  44.0   \n",
       "3                    19.0                   2.0   \n",
       "4                     5.0                   1.0   \n",
       "...                   ...                   ...   \n",
       "10234                 1.0                   0.0   \n",
       "10235                 1.0                   0.0   \n",
       "10236                 3.0                   3.0   \n",
       "10237                 0.0                   0.0   \n",
       "10238                 0.0                   2.0   \n",
       "\n",
       "                                                 context  \n",
       "0                                        a floor speech.  \n",
       "1                                                 Denver  \n",
       "2                                         a news release  \n",
       "3                                    an interview on CNN  \n",
       "4                              a an online opinion-piece  \n",
       "...                                                  ...  \n",
       "10234                  interview on \"The Colbert Report\"  \n",
       "10235                                       an interview  \n",
       "10236                   a Republican presidential debate  \n",
       "10237  a televised debate on Miami's WPLG-10 against ...  \n",
       "10238                               a Fox News interview  \n",
       "\n",
       "[10239 rows x 14 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7f7938-34bb-4756-aad5-881c604de2d7",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c801c425-32e5-4871-aa49-4f9963e5aa3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'label', 'statement', 'subject', 'speaker', 'speaker_job',\n",
      "       'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts',\n",
      "       'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts',\n",
      "       'context'],\n",
      "      dtype='object')\n",
      "Index(['id', 'label', 'statement', 'subject', 'speaker', 'speaker_job',\n",
      "       'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts',\n",
      "       'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts',\n",
      "       'context'],\n",
      "      dtype='object')\n",
      "Index(['id', 'label', 'statement', 'subject', 'speaker', 'speaker_job',\n",
      "       'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts',\n",
      "       'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts',\n",
      "       'context'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train_df.columns)\n",
    "print(test_df.columns)\n",
    "print(valid_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f2b344-6e9e-41a1-b6e5-52046175be20",
   "metadata": {},
   "source": [
    "### Define preprocessing functions\n",
    "\n",
    "Here, we define the functions that we are going to apply to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a227e891-a2b3-4002-b4d0-9c0ae4464e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['half-true', 'mostly-true', 'false', 'true', 'barely-true',\n",
       "       'pants-fire'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cb58177-fa94-4928-b84e-b0646651e522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str) -> str:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    text = text.lower()\n",
    "    # TLDR; translate is just a fancier replace method\n",
    "    # The translate() method returns a string where some specified characters are replaced \n",
    "    # with the character described in a dictionary, or in a mapping table.\n",
    "    # The maketrans() method returns a mapping table that can be used with the translate() \n",
    "    # method to replace specified characters. The third parameter is a string describing what characters to remove.\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    processed_text = \" \".join(tokens)\n",
    "    return processed_text\n",
    "\n",
    "def numero_labelo(label: str) -> str:\n",
    "    # binary labeling because it's not fake if it's half true\n",
    "    return 1 if label in [\"half-true\", \"mostly-true\", \"true\"] else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c3ac2c4-42f5-4c1b-a85f-255ca71c369b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bloodymindedness understanding analog digital wont go unable able youll go unable able progressively long remain bloodyminded continually working\n",
      "waste time saying lot word word trick\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "print(preprocess_text(text=\"Bloody-mindedness, and the understanding that it's analog, not digital. \\\n",
    "                        You won't go from unable to able; you'll go from unable to more able, progressively, \\\n",
    "                        for as long as you remain bloody-minded about continually working on it.\"))\n",
    "\n",
    "# Quote by Kevin Malone (from what he intended)\n",
    "print(preprocess_text(\"Why waste time saying a lot of words when few words can do the trick?\"))\n",
    "\n",
    "# Convert Label example\n",
    "print(numero_labelo(\"half-true\"))\n",
    "print(numero_labelo(\"barely-true\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45462415-f15c-4978-aaff-8f13f92d8c59",
   "metadata": {},
   "source": [
    "### Applying the methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dda2e55-45d5-476a-8037-95467e8299a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['processed_statement'] = train_df['statement'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a303e5b-94c3-4a44-8664-06be1521d781",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['label_encoded'] = train_df['label'].apply(numero_labelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307b43ad-bf93-4eb5-afa8-08447b36aaa4",
   "metadata": {},
   "source": [
    "#### Apply to test and validation datasets as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "307c2282-213b-44ea-b013-a9b7edfd1b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['processed_statement'] = test_df['statement'].apply(preprocess_text)\n",
    "test_df['label_encoded'] = test_df['label'].apply(numero_labelo)\n",
    "valid_df['processed_statement'] = valid_df['statement'].apply(preprocess_text)\n",
    "valid_df['label_encoded'] = valid_df['label'].apply(numero_labelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abff9ea8-0d7b-4703-996b-4b17af668000",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61085a8b-2dee-410e-ac8e-737eff028456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge statements\n",
    "all_statements = pd.concat([train_df['processed_statement'], valid_df['processed_statement']])\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(all_statements)\n",
    "\n",
    "X_train = vectorizer.fit_transform(train_df['processed_statement'])\n",
    "X_valid = vectorizer.transform(valid_df['processed_statement'])\n",
    "X_test = vectorizer.transform(test_df['processed_statement'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506e393a-c4c9-45fd-a8de-6cb4babd8261",
   "metadata": {},
   "source": [
    "## Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31bb350e-ad38-401f-91d2-a14a335af5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.6102883865939205\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.46      0.53       615\n",
      "           1       0.60      0.75      0.67       668\n",
      "\n",
      "    accuracy                           0.61      1283\n",
      "   macro avg       0.61      0.60      0.60      1283\n",
      "weighted avg       0.61      0.61      0.60      1283\n",
      "\n",
      "Test accuracy: 0.6192733017377567\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.44      0.50       553\n",
      "           1       0.64      0.76      0.69       713\n",
      "\n",
      "    accuracy                           0.62      1266\n",
      "   macro avg       0.61      0.60      0.60      1266\n",
      "weighted avg       0.61      0.62      0.61      1266\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, train_df['label_encoded'])\n",
    "\n",
    "# validate model\n",
    "y_valid_pred = model.predict(X_valid)\n",
    "print(f\"Validation accuracy: {accuracy_score(valid_df['label_encoded'], y_valid_pred)}\")\n",
    "print(classification_report(valid_df['label_encoded'], y_valid_pred))\n",
    "\n",
    "# test the model\n",
    "y_test_pred = model.predict(X_test)\n",
    "print(f\"Test accuracy: {accuracy_score(test_df['label_encoded'], y_test_pred)}\")\n",
    "print(classification_report(test_df['label_encoded'], y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56af0c07-b79c-46eb-9618-6ff4066c3d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set performance:  0.5900233826968043\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.31      0.42       615\n",
      "           1       0.57      0.84      0.68       668\n",
      "\n",
      "    accuracy                           0.59      1283\n",
      "   macro avg       0.61      0.58      0.55      1283\n",
      "weighted avg       0.61      0.59      0.56      1283\n",
      "\n",
      "Test set performance:  0.6058451816745656\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.28      0.38       553\n",
      "           1       0.61      0.86      0.71       713\n",
      "\n",
      "    accuracy                           0.61      1266\n",
      "   macro avg       0.61      0.57      0.55      1266\n",
      "weighted avg       0.61      0.61      0.57      1266\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, train_df['label_encoded'])\n",
    "\n",
    "y_valid_pred = nb_classifier.predict(X_valid)\n",
    "print(\"Validation set performance: \", accuracy_score(valid_df['label_encoded'], y_valid_pred))\n",
    "print(classification_report(valid_df['label_encoded'], y_valid_pred))\n",
    "\n",
    "y_test_pred = nb_classifier.predict(X_test)\n",
    "print(\"Test set performance: \", accuracy_score(test_df['label_encoded'], y_test_pred))\n",
    "print(classification_report(test_df['label_encoded'], y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c440f8d-64c7-430d-b6b9-d7558d5e411d",
   "metadata": {},
   "source": [
    "## Is the dataset biased?\n",
    "\n",
    "Since the dataset is already provided splitted into train, test, and validation, could it be splitted in bias?\n",
    "\n",
    "Here, we're going to concat them and split them again randomly to see if they yield better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae04ac6f-8e62-4bbd-bbd9-d817575fb869",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df, valid_df = read_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48fc2e32-79fb-4d1f-b03f-db0fe442eb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10239, 14)\n",
      "(1266, 14)\n",
      "(1283, 14)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "print(test_df.shape)\n",
    "print(valid_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "414255d5-0c0e-4e9c-986c-13bd9c4fe9f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12788, 14)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df = pd.concat([train_df, test_df, valid_df], ignore_index=True)\n",
    "\n",
    "all_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71aee7e6-40db-4665-b413-572c0dc0e8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = all_df['statement'].apply(preprocess_text)\n",
    "y = all_df['label'].apply(numero_labelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a392418a-e41e-499d-92e4-ec62ed94101a",
   "metadata": {},
   "source": [
    "### Split Ratio\n",
    "\n",
    "We're going to split the dataset into 8:1:1 ratio.\n",
    "\n",
    "- 80% for training\n",
    "- 10% for validation\n",
    "- 10% for testing\n",
    "\n",
    "[Answer to life, the universe, and everything](https://www.google.com/search?q=what+is+the+answer+to+life+the+universe+and+everything&sca_esv=0f377be67df6cd70&sxsrf=ADLYWIJ-zE-JGgkX3L96BTR--kiHVhcnvw%3A1720336223275&ei=Xz-KZobCEOnF1e8Pnry2yAc&oq=what+is+the+answer+to+life%2C+&gs_lp=Egxnd3Mtd2l6LXNlcnAiHHdoYXQgaXMgdGhlIGFuc3dlciB0byBsaWZlLCAqAggAMgoQABiABBgUGIcCMgoQABiABBgUGIcCMgoQABiABBhDGIoFMgUQABiABDIFEAAYgAQyBRAAGIAEMgUQABiABDIFEAAYgAQyBRAAGIAEMgUQABiABEjZOlD1FVijNXACeAGQAQCYAbUBoAGkIaoBBDAuMjm4AQPIAQD4AQGYAh-gAtIiwgIKEAAYsAMY1gQYR8ICDRAAGIAEGLADGEMYigXCAg4QABiwAxjkAhjWBNgBAcICExAuGIAEGLADGEMYyAMYigXYAQLCAhYQLhiABBiwAxhDGNQCGMgDGIoF2AECwgIKECMYgAQYJxiKBcICBBAjGCfCAg0QLhiABBhDGNQCGIoFwgIQEAAYgAQYsQMYQxiDARiKBcICCxAAGIAEGLEDGIMBwgIIEAAYgAQYsQPCAgUQLhiABMICEBAAGIAEGLEDGEMYyQMYigXCAgsQABiABBiSAxiKBcICChAuGIAEGEMYigXCAgsQABiABBiRAhiKBZgDAIgGAZAGELoGBggBEAEYCboGBggCEAEYCJIHBDIuMjmgB4GYAg&sclient=gws-wiz-serp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5bbddbfa-ecc6-4e55-99ef-103a8551958f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 12788\n",
      "Train: 10230.400000000001\n",
      "Test: 1278.8000000000002\n",
      "Valid: 1278.8000000000002\n",
      "\n",
      "X_train shape: 10230\n",
      "X_temp shape: 2558\n",
      "y_train shape: 10230\n",
      "y_temp shape: 2558\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total: {all_df.shape[0]}\")\n",
    "print(f\"Train: {all_df.shape[0] * 0.8}\")\n",
    "print(f\"Test: {all_df.shape[0] * 0.1}\")\n",
    "print(f\"Valid: {all_df.shape[0] * 0.1}\", end=\"\\n\\n\")\n",
    "\n",
    "# 20% because test(10%) + validation(10%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=37)\n",
    "print(f\"X_train shape: {len(X_train)}\")\n",
    "print(f\"X_temp shape: {len(X_temp)}\")\n",
    "print(f\"y_train shape: {len(y_train)}\")\n",
    "print(f\"y_temp shape: {len(y_temp)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e06ea5ab-3272-4aa8-bedb-c37c94b6c0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 10230\n",
      "Testing set size: 1279\n",
      "Validation set size: 1279\n"
     ]
    }
   ],
   "source": [
    "# again split the 20% to test and valid by half (get 10% & 10%)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=37)\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Testing set size: {len(X_test)}\")\n",
    "print(f\"Validation set size: {len(X_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f1ff87e-cfdb-4fdb-b19a-40df72802cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_temp, y_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadb9393-bfd5-4032-b013-298d45230cc5",
   "metadata": {},
   "source": [
    "### Vectorization (again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32a7d59a-c849-402f-a719-f875acbbf9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_valid_vec = vectorizer.transform(X_val)\n",
    "X_test_vec = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1191dc19-e75d-435a-a427-55f187308454",
   "metadata": {},
   "source": [
    "### Models (again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5fd06dd2-7901-4af6-90a9-b083da0eca20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.6309616888193902\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.49      0.53       542\n",
      "           1       0.66      0.74      0.70       737\n",
      "\n",
      "    accuracy                           0.63      1279\n",
      "   macro avg       0.62      0.61      0.61      1279\n",
      "weighted avg       0.63      0.63      0.63      1279\n",
      "\n",
      "Test accuracy: 0.6067240031274433\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.45      0.50       545\n",
      "           1       0.64      0.72      0.68       734\n",
      "\n",
      "    accuracy                           0.61      1279\n",
      "   macro avg       0.59      0.59      0.59      1279\n",
      "weighted avg       0.60      0.61      0.60      1279\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "# validate model\n",
    "y_valid_pred = model.predict(X_valid_vec)\n",
    "print(f\"Validation accuracy: {accuracy_score(y_val, y_valid_pred)}\")\n",
    "print(classification_report(y_val, y_valid_pred))\n",
    "\n",
    "# test the model\n",
    "y_test_pred = model.predict(X_test_vec)\n",
    "print(f\"Test accuracy: {accuracy_score(y_test, y_test_pred)}\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "249fa26b-7ebf-418a-b520-59546932dca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set performance:  0.6043784206411259\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.31      0.40       542\n",
      "           1       0.62      0.82      0.71       737\n",
      "\n",
      "    accuracy                           0.60      1279\n",
      "   macro avg       0.59      0.57      0.55      1279\n",
      "weighted avg       0.59      0.60      0.58      1279\n",
      "\n",
      "Test set performance:  0.6106333072713057\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.32      0.42       545\n",
      "           1       0.62      0.82      0.71       734\n",
      "\n",
      "    accuracy                           0.61      1279\n",
      "   macro avg       0.60      0.57      0.56      1279\n",
      "weighted avg       0.60      0.61      0.58      1279\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_vec, y_train)\n",
    "\n",
    "y_valid_pred = nb_classifier.predict(X_valid_vec)\n",
    "print(\"Validation set performance: \", accuracy_score(y_val, y_valid_pred))\n",
    "print(classification_report(y_val, y_valid_pred))\n",
    "\n",
    "y_test_pred = nb_classifier.predict(X_test_vec)\n",
    "print(\"Test set performance: \", accuracy_score(y_test, y_test_pred))\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7056dc79-e0da-4c42-af05-810502e64790",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
